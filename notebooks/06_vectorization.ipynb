{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb961fc",
   "metadata": {},
   "source": [
    "Так как надо протестировать все модели по несколько раз (без K-Fold, с K-Fold, с K-Fold и SMOTE), можно заранее векторизовать датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d29471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587e2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/processed/rule_classified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa3af47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer\n",
       "Отказ                                     217\n",
       "Частично удовлетворено                    138\n",
       "Обращение рассмотрено                     130\n",
       "Взыскание обращено                        106\n",
       "Запрос направлен                           80\n",
       "Возбуждено исполнительное производство     76\n",
       "Постановление вынесено                     37\n",
       "Удовлетворено                              27\n",
       "Заявления и жалобы рассматриваются         26\n",
       "Объявлен исполнительный розыск             20\n",
       "Применены меры для исполнения              19\n",
       "Запрет действий                            14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = df[df['answer'].notna()].copy()\n",
    "df_labeled = df_labeled.reset_index(drop=True)\n",
    "\n",
    "df_labeled['answer'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7a8035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classification_utils import split_data\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(df_labeled)\n",
    "vectorizers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46aa41e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "def save_vec_model_pkl(model, model_name):\n",
    "    with open(f'models/vectorization/{model_name}_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "def save_vec_np_arrays(X_train, X_test, model_name, np_type):\n",
    "    folder_path = Path(f'vectors/{model_name}')\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if np_type == 'npz':\n",
    "        train_path = f'vectors/{model_name}/X_train_{model_name}.npz'\n",
    "        test_path = f'vectors/{model_name}/X_test_{model_name}.npz'\n",
    "        save_npz(train_path, X_train)\n",
    "        save_npz(test_path, X_test)\n",
    "    elif np_type == 'npy':\n",
    "        train_path = f'vectors/{model_name}/X_train_{model_name}.npy'\n",
    "        test_path = f'vectors/{model_name}/X_test_{model_name}.npy'\n",
    "        np.save(train_path, X_train)\n",
    "        np.save(test_path, X_test)\n",
    "    else:\n",
    "        raise ValueError('np_type должен быть \"npz\" или \"npy\"')\n",
    "    \n",
    "    return train_path, test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325525cc",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "035d51c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "tf_idf_model = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_train_tf_idf = tf_idf_model.fit_transform(X_train)\n",
    "X_test_tf_idf = tf_idf_model.transform(X_test)\n",
    "\n",
    "save_vec_model_pkl(tf_idf_model, 'tf_idf')\n",
    "X_train_tf_idf_path, X_test_tf_idf_path = save_vec_np_arrays(X_train_tf_idf, X_test_tf_idf, 'tf_idf', 'npz')\n",
    "\n",
    "vectorizers['tf_idf'] = (X_train_tf_idf_path, X_test_tf_idf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47093ab",
   "metadata": {},
   "source": [
    "### Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ef3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hashvec_model = HashingVectorizer(n_features=1000, ngram_range=(1, 2))\n",
    "X_train_hashvec = hashvec_model.transform(X_train)\n",
    "X_test_hashvec = hashvec_model.transform(X_test)\n",
    "\n",
    "X_train_hashvec_path, X_test_hashvec_path = save_vec_np_arrays(X_train_hashvec, X_test_hashvec, 'hashvec', 'npz')\n",
    "\n",
    "vectorizers['hashvec'] = (X_train_hashvec_path, X_test_hashvec_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17073e91",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c738bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=[text.split() for text in X_train], \n",
    "                     vector_size=100, window=5, min_count=1)\n",
    "\n",
    "def text_to_word2vec(texts, model, vector_size=100):\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        word_vectors = [model.wv[w] for w in words if w in model.wv]\n",
    "        if word_vectors:\n",
    "            vectors.append(np.mean(word_vectors, axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(vector_size))\n",
    "    return np.array(vectors)\n",
    "\n",
    "X_train_word2vec = text_to_word2vec(X_train, word2vec_model, 100)\n",
    "X_test_word2vec = text_to_word2vec(X_test, word2vec_model, 100)\n",
    "\n",
    "X_train_word2vec_path, X_test_word2vec_path = save_vec_np_arrays(X_train_word2vec, X_test_word2vec, 'word2vec', 'npy')\n",
    "\n",
    "vectorizers['word2vec'] = (X_train_word2vec_path, X_test_word2vec_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87fb57a",
   "metadata": {},
   "source": [
    "Можно было бы обучить модель, но датасет слишком маленький"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd1916ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script cmd /c \"\"\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Word2Vec с обучением\n",
    "tokenized_train = [text.split() for text in X_train]\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_train,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=10,\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "def text_to_w2v(texts, model):\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        word_vectors = [model.wv[w] for w in words if w in model.wv]\n",
    "        if word_vectors:\n",
    "            vectors.append(np.mean(word_vectors, axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(100))\n",
    "    return np.array(vectors)\n",
    "\n",
    "X_train_w2v_fitted = text_to_w2v(X_train, w2v_model)\n",
    "X_test_w2v_fitted = text_to_w2v(X_test, w2v_model)\n",
    "\n",
    "create_vectors_folder('w2v_fitted')\n",
    "\n",
    "np.save('vectors/w2v/X_train_w2v_fitted.npy', X_train_w2v_fitted)\n",
    "np.save('vectors/w2v/X_test_w2v_fitted.npy', X_test_w2v_fitted)\n",
    "\n",
    "vectorizers['word2vec_fitten'] = ('vectors/w2v/X_train_w2v_fitted.npy', 'vectors/w2v/X_test_w2v_fitted.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af9a13",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "970e8a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "tagged_train = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(X_train)]\n",
    "\n",
    "doc2vec_model = Doc2Vec(vector_size=100, min_count=1, epochs=40)\n",
    "doc2vec_model.build_vocab(tagged_train)\n",
    "doc2vec_model.train(tagged_train, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "def text_to_doc2vec(texts, model):\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        vectors.append(model.infer_vector(text.split()))\n",
    "    return np.array(vectors)\n",
    "\n",
    "X_train_doc2vec = text_to_doc2vec(X_train, doc2vec_model)\n",
    "X_test_doc2vec = text_to_doc2vec(X_test, doc2vec_model)\n",
    "\n",
    "save_vec_model_pkl(doc2vec_model, 'doc2vec')\n",
    "X_train_doc2vec_path, X_test_doc2vec_path = save_vec_np_arrays(X_train_doc2vec, X_test_doc2vec, 'doc2vec', 'npy')\n",
    "\n",
    "vectorizers['doc2vec'] = (X_train_doc2vec_path, X_test_doc2vec_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc610c2",
   "metadata": {},
   "source": [
    "### FastText предобученный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da04f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "# Кинуть в папку models/vectorization/ https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "fasttext_model = load_facebook_model('models/vectorization/cc.ru.300.bin')\n",
    "\n",
    "def text_to_fasttext(texts, model, vector_size=100):\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        word_vectors = [model.wv[w] for w in words]\n",
    "        if word_vectors:\n",
    "            vectors.append(np.mean(word_vectors, axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(vector_size))\n",
    "    return np.array(vectors)\n",
    "\n",
    "X_train_fasttext = text_to_fasttext(X_train, fasttext_model, 100)\n",
    "X_test_fasttext = text_to_fasttext(X_test, fasttext_model, 100)\n",
    "\n",
    "X_train_fasttext_path, X_test_fasttext_path = save_vec_np_arrays(X_train_fasttext, X_test_fasttext, 'fasttext', 'npy')\n",
    "\n",
    "vectorizers['fasttext'] = (X_train_fasttext_path, X_test_fasttext_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe30024",
   "metadata": {},
   "source": [
    "Можно было бы обучить модель, но датасет слишком маленький"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1a1b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script cmd /c \"\"\n",
    "from gensim.models import FastText\n",
    "\n",
    "# FastText с обучением\n",
    "tokenized_train = [text.split() for text in X_train]\n",
    "\n",
    "ft_model = FastText(\n",
    "    sentences=tokenized_train,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=10,\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "def text_to_fasttext(texts, model, vector_size=100):\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        word_vectors = [model.wv[w] for w in words]\n",
    "        if word_vectors:\n",
    "            vectors.append(np.mean(word_vectors, axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(vector_size))\n",
    "    return np.array(vectors)\n",
    "\n",
    "X_train_ft_fitted = text_to_fasttext(X_train, ft_model, 100)\n",
    "X_test_ft_fitted = text_to_fasttext(X_test, ft_model, 100)\n",
    "\n",
    "create_vectors_folder('ft')\n",
    "\n",
    "np.save('vectors/ft/X_train_ft_fitted.npy', X_train_ft_fitted)\n",
    "np.save('vectors/ft/X_test_ft_fitted.npy', X_test_ft_fitted)\n",
    "# ft_model.save('fasttext_fitted.model')\n",
    "\n",
    "vectorizers['fasttext_fitted'] = ('vectors/ft/X_train_ft_fitted.npy', 'vectors/ft/X_test_ft_fitted.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb86c944",
   "metadata": {},
   "source": [
    "### Navec (GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc11483",
   "metadata": {},
   "source": [
    "Новее моделей RusVectores и Navec 2020 года нету. Navec показывает лучшие метрики с меньшим размером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b823d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec\n",
    "\n",
    "# Кинуть в папку models/vectorization/ https://github.com/natasha/navec\n",
    "navec_path = 'models/vectorization/navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
    "navec = Navec.load(navec_path)\n",
    "\n",
    "def text_to_navec(texts, navec_model):\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        word_vectors = [navec_model[v] for v in words if v in navec_model]\n",
    "        if word_vectors:\n",
    "            vectors.append(np.mean(word_vectors, axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(navec_model.dims))\n",
    "    return np.array(vectors)\n",
    "\n",
    "X_train_navec = text_to_navec(X_train, navec)\n",
    "X_test_navec = text_to_navec(X_test, navec)\n",
    "\n",
    "X_train_navec_path, X_test_navec_path = save_vec_np_arrays(X_train_navec, X_test_navec, 'navec', 'npy')\n",
    "\n",
    "vectorizers['glove'] = (X_train_navec_path, X_test_navec_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15da297",
   "metadata": {},
   "source": [
    "### ruBERT (проблема, что нету uncased популярной)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76276842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972ed62948a147cdb621d6881649fa9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121a30210752432ab05f275f5da29b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "rusbert_model = SentenceTransformer('sergeyzh/rubert-mini-uncased')\n",
    "\n",
    "X_train_rusbert = rusbert_model.encode(list(X_train), show_progress_bar=True)\n",
    "X_test_rusbert = rusbert_model.encode(list(X_test), show_progress_bar=True)\n",
    "\n",
    "X_train_rusbert_path, X_test_rusbert_path = save_vec_np_arrays(X_train_rusbert, X_test_rusbert, 'sbert', 'npy')\n",
    "\n",
    "vectorizers['sbert'] = (X_train_rusbert_path, X_test_rusbert_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a22bad",
   "metadata": {},
   "source": [
    "### Сохранение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b43899",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vectors/vectorizers.pkl', 'wb') as f:\n",
    "    import pickle\n",
    "    pickle.dump(vectorizers, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
